{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification. Linear models and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import plot_confusion_matrix, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementing Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you need to implement Logistic Regression with l2 regularization using gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression loss:\n",
    "$$ L(w) = \\dfrac{1}{N}\\sum_{i=1}^N \\log(1 + e^{-\\langle w, x_i \\rangle y_i}) + \\frac{1}{2C} \\lVert w \\rVert^2  \\to \\min_w$$\n",
    "$$\\langle w, x_i \\rangle = \\sum_{j=1}^n w_{j}x_{ij} + w_{0},$$ $$ y_{i} \\in \\{-1, 1\\}$$ where $n$ is the number of features and $N$ is the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent step:\n",
    "$$w^{(t+1)} := w^{(t)} + \\dfrac{\\eta}{N}\\sum_{i=1}^N y_ix_i \\Big(1 - \\dfrac{1}{1 + exp(-\\langle w^{(t)}, x_i \\rangle y_i)}\\Big) - \\eta \\frac{1}{C} w,$$\n",
    "where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2 points)** Implement the algorithm and use it to classify the digits (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) into \"even\" and \"odd\" categories. \"Even\" and \"Odd\" classes  should correspond to {-1, 1} labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopping criteria: either the number of iterations exceeds *max_iter* or $||w^{(t+1)} - w^{(t)}||_2 < tol$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import NotFittedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogisticRegression:\n",
    "    _estimator_type = \"classifier\"\n",
    "    \n",
    "    def __init__(self, eta=0.001, max_iter=1000, C=1.0, tol=1e-5, random_state=42, zero_init=False):\n",
    "        \"\"\"Logistic Regression classifier.\n",
    "        \n",
    "        Args:\n",
    "            eta: float, default=0.001\n",
    "                Learning rate.\n",
    "            max_iter: int, default=1000\n",
    "                Maximum number of iterations taken for the solvers to converge.\n",
    "            C: float, default=1.0\n",
    "                Inverse of regularization strength; must be a positive float.\n",
    "                Smaller values specify stronger regularization.\n",
    "            tol: float, default=1e-5\n",
    "                Tolerance for stopping criteria.\n",
    "            random_state: int, default=42\n",
    "                Random state.\n",
    "            zero_init: bool, default=False\n",
    "                Zero weight initialization.\n",
    "        \"\"\"\n",
    "        self.eta = eta\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        self.tol = tol\n",
    "        self.random_state = np.random.RandomState(seed=random_state)\n",
    "        self.zero_init = zero_init\n",
    "        self.loss_values = []\n",
    "         \n",
    "    def get_sigmoid(self, X, weights):\n",
    "        \"\"\"Compute the sigmoid value.\"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-np.dot(X, weights)))\n",
    "    \n",
    "    def get_loss(self, x, weights, y):\n",
    "        \"\"\"Calculate the loss.\"\"\"\n",
    "        temp = []\n",
    "        for j in range(x.shape[0]):\n",
    "            q =  np.log(1 + np.exp(-weights[1:] @ x[j] * y[j]))\n",
    "            temp.append(q)\n",
    "        loss = np.sum(temp, axis=0) / x.shape[0] + weights.T @ weights / (2 * self.C)\n",
    "\n",
    "        return loss\n",
    "     \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Target vector.        \n",
    "        \"\"\"\n",
    "        X_ext = np.hstack([np.ones((X.shape[0], 1)), X]) # a constant feature is included to handle intercept\n",
    "        num_features = X_ext.shape[1]\n",
    "        if self.zero_init:\n",
    "            self.weights_ = np.zeros(num_features) \n",
    "        else:\n",
    "            weight_threshold = 1.0 / (2 * num_features)\n",
    "            self.weights_ = self.random_state.uniform(low=-weight_threshold,\n",
    "                                                      high=weight_threshold, size=num_features) # random weight initialization\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            self.loss_values.append(self.get_loss(X, self.weights_, y))\n",
    "            l_slog = []\n",
    "            for ii in range(y.shape[0]):\n",
    "                list_slog = y[ii] * X_ext[ii, :] * (1 - 1 / (1 + np.exp(-X_ext[ii, :] @ self.weights_ * y[ii])))\n",
    "                l_slog.append(list_slog)\n",
    "            delta = -np.sum(np.array(l_slog), axis=0) / X_ext.shape[0] + self.weights_/self.C\n",
    "            self.weights_ -= self.eta * delta\n",
    "            w_t = self.weights_ + self.eta * delta\n",
    "            if (np.linalg.norm(w_t - self.weights_) < self.tol):\n",
    "                break\n",
    "    \n",
    "    def return_get_loss():\n",
    "        return self.loss_values\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict positive class probabilities.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Vector containing positive class probabilities.\n",
    "        \"\"\"\n",
    "        X_ext = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        if hasattr(self, 'weights_'):\n",
    "            return self.get_sigmoid(X_ext, self.weights_)\n",
    "        else: \n",
    "            raise NotFittedError(\"CustomLogisticRegression instance is not fitted yet\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Vector containing predicted class labels.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'weights_'):\n",
    "            cl = self.predict_proba(X)\n",
    "            return np.where(cl < 0.5, -1, 1)\n",
    "        else:\n",
    "            raise NotFittedError('is not fitted yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 21 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = datasets.load_digits(n_class=10, return_X_y=True)\n",
    "\n",
    "_, axes = plt.subplots(nrows=3, ncols=7, figsize=(10, 5))\n",
    "for ax, image, label in zip(axes.flatten(), X, y):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image.reshape((8, 8)), cmap=plt.cm.gray_r if label % 2 else plt.cm.afmhot_r)\n",
    "    ax.set_title(label)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "#y_train = \"<your code>\"\n",
    "#y_test = \"<your code>\"\n",
    "y_train = (y_train % 2) * 2 - 1\n",
    "y_test = (y_test % 2) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (np.unique(y_train) == [-1, 1]).all()\n",
    "assert (np.unique(y_test) == [-1, 1]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_evaluate(clf, X_train, y_train, X_test, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    disp = metrics.plot_confusion_matrix(clf, X_test, y_test, normalize='true')\n",
    "    disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics.accuracy_score(y_pred=clf.predict(X_train), y_true=y_train), \\\n",
    "           metrics.accuracy_score(y_pred=clf.predict(X_test), y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = CustomLogisticRegression(max_iter=1, zero_init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(lr_clf.get_sigmoid(np.array([[0.5, 0, 1.0], [0.3, 1.3, 1.0]]), np.array([0.5, -0.5, 0.1])),\n",
    "                   np.array([0.58662, 0.40131]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(lr_clf.weights_, np.array([ 3.1000e-06,  0.0000e+00,  4.1800e-05,  5.4770e-04,  2.2130e-04,\n",
    "        4.8750e-04,  1.3577e-03,  5.9780e-04,  5.6400e-05, -7.0000e-07,\n",
    "        1.6910e-04,  2.5190e-04, -4.3700e-04,  3.6190e-04,  1.0049e-03,\n",
    "        4.2280e-04,  2.5700e-05,  3.0000e-07, -1.1500e-05, -7.2440e-04,\n",
    "       -2.6200e-04,  8.7540e-04,  4.1540e-04, -8.4200e-05, -5.2000e-06,\n",
    "        0.0000e+00, -2.2160e-04, -5.7130e-04,  9.8570e-04,  1.3507e-03,\n",
    "        5.0210e-04, -1.7050e-04, -1.0000e-06,  0.0000e+00, -6.7810e-04,\n",
    "       -1.0515e-03, -4.4500e-05,  3.7160e-04,  4.2100e-04, -8.1800e-05,\n",
    "        0.0000e+00, -5.2000e-06, -5.3410e-04, -2.0393e-03, -8.4310e-04,\n",
    "        1.0400e-04, -1.2390e-04, -1.7880e-04, -1.3200e-05, -4.5000e-06,\n",
    "       -9.4300e-05, -1.1127e-03, -5.0900e-04, -2.1850e-04, -5.6050e-04,\n",
    "       -3.9560e-04, -1.7700e-05, -3.0000e-07,  2.6800e-05,  6.3920e-04,\n",
    "        1.8090e-04, -7.3660e-04, -5.3930e-04, -3.7060e-04, -2.8200e-05]), atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MiniConda\\envs\\rsschool-machine-learning-course\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_acc, test_acc = fit_evaluate(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9109255393180237, 0.9388888888888889)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert min(train_acc, test_acc) > 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Visualize the loss history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7610240405680223,\n",
       " 0.7338666123760155,\n",
       " 0.7106671667907737,\n",
       " 0.6894546943461709,\n",
       " 0.6698860213011825,\n",
       " 0.651806915977628,\n",
       " 0.6350895650932957,\n",
       " 0.6196160331165909,\n",
       " 0.6052770529648284,\n",
       " 0.5919720195405751,\n",
       " 0.5796088391292645,\n",
       " 0.5681035934758148,\n",
       " 0.5573800803616614,\n",
       " 0.5473692835817501,\n",
       " 0.5380088110905343,\n",
       " 0.529242328807615,\n",
       " 0.521019008897472,\n",
       " 0.513293004738452,\n",
       " 0.5060229598763876,\n",
       " 0.4991715546852664,\n",
       " 0.4927050919410723,\n",
       " 0.4865931208072388,\n",
       " 0.480808097626129,\n",
       " 0.47532508124690637,\n",
       " 0.4701214602688917,\n",
       " 0.4651767094446161,\n",
       " 0.46047217249676475,\n",
       " 0.45599086870599964,\n",
       " 0.4517173207851374,\n",
       " 0.4476374017434028,\n",
       " 0.4437381986447713,\n",
       " 0.44000789136504787,\n",
       " 0.43643564464586637,\n",
       " 0.43301151192578075,\n",
       " 0.4297263495967048,\n",
       " 0.4265717404871581,\n",
       " 0.4235399255120579,\n",
       " 0.42062374255267587,\n",
       " 0.41781657174073117,\n",
       " 0.4151122864184725,\n",
       " 0.41250520913312644,\n",
       " 0.4099900721003986,\n",
       " 0.407561981638876,\n",
       " 0.40521638613622696,\n",
       " 0.4029490471599578,\n",
       " 0.40075601337100575,\n",
       " 0.39863359693841244,\n",
       " 0.3965783521883819,\n",
       " 0.39458705625181834,\n",
       " 0.39265669150146465,\n",
       " 0.39078442959351634,\n",
       " 0.38896761694946463,\n",
       " 0.38720376153228786,\n",
       " 0.38549052078728846,\n",
       " 0.38382569063211536,\n",
       " 0.38220719539309,\n",
       " 0.38063307859604417,\n",
       " 0.379101494529685,\n",
       " 0.3776107005081824,\n",
       " 0.37615904976734776,\n",
       " 0.374744984935589,\n",
       " 0.37336703202686883,\n",
       " 0.37202379490825843,\n",
       " 0.3707139501994577,\n",
       " 0.3694362425659033,\n",
       " 0.36818948037087185,\n",
       " 0.36697253165537114,\n",
       " 0.365784320417628,\n",
       " 0.36462382316668573,\n",
       " 0.3634900657270379,\n",
       " 0.36238212027339495,\n",
       " 0.36129910257662134,\n",
       " 0.3602401694436276,\n",
       " 0.3592045163355705,\n",
       " 0.3581913751501255,\n",
       " 0.35720001215486974,\n",
       " 0.35622972605995895,\n",
       " 0.3552798462193174,\n",
       " 0.3543497309504968,\n",
       " 0.3534387659642041,\n",
       " 0.352546362895266,\n",
       " 0.35167195792749006,\n",
       " 0.35081501050551434,\n",
       " 0.34997500212730576,\n",
       " 0.34915143521148967,\n",
       " 0.3483438320341648,\n",
       " 0.3475517337302844,\n",
       " 0.3467746993550818,\n",
       " 0.34601230500137053,\n",
       " 0.3452641429688787,\n",
       " 0.34452982098207274,\n",
       " 0.3438089614531993,\n",
       " 0.34310120078752204,\n",
       " 0.34240618872796014,\n",
       " 0.3417235877365401,\n",
       " 0.34105307241026955,\n",
       " 0.34039432892921323,\n",
       " 0.33974705453471477,\n",
       " 0.33911095703585603,\n",
       " 0.33848575434238315,\n",
       " 0.3378711740224519,\n",
       " 0.33726695288366365,\n",
       " 0.3366728365759681,\n",
       " 0.33608857921510743,\n",
       " 0.335513943025369,\n",
       " 0.3349486980004953,\n",
       " 0.33439262158167976,\n",
       " 0.33384549835164745,\n",
       " 0.33330711974388644,\n",
       " 0.332777283766157,\n",
       " 0.3322557947374633,\n",
       " 0.33174246303772503,\n",
       " 0.3312371048694341,\n",
       " 0.33073954203062955,\n",
       " 0.3302496016985641,\n",
       " 0.3297671162234755,\n",
       " 0.32929192293191295,\n",
       " 0.32882386393910285,\n",
       " 0.3283627859698684,\n",
       " 0.32790854018765025,\n",
       " 0.32746098203119756,\n",
       " 0.3270199710585319,\n",
       " 0.32658537079780203,\n",
       " 0.3261570486046776,\n",
       " 0.325734875525944,\n",
       " 0.32531872616898666,\n",
       " 0.3249084785768644,\n",
       " 0.3245040141086946,\n",
       " 0.3241052173250851,\n",
       " 0.3237119758783639,\n",
       " 0.32332418040737226,\n",
       " 0.32294172443659835,\n",
       " 0.3225645042794435,\n",
       " 0.3221924189454208,\n",
       " 0.32182537005110085,\n",
       " 0.3214632617346271,\n",
       " 0.3211060005736314,\n",
       " 0.3207534955063949,\n",
       " 0.3204056577561002,\n",
       " 0.3200624007580361,\n",
       " 0.31972364008961696,\n",
       " 0.3193892934030919,\n",
       " 0.31905928036082076,\n",
       " 0.3187335225730028,\n",
       " 0.31841194353774854,\n",
       " 0.318094468583393,\n",
       " 0.3177810248129492,\n",
       " 0.31747154105061126,\n",
       " 0.31716594779021695,\n",
       " 0.31686417714558635,\n",
       " 0.31656616280265504,\n",
       " 0.3162718399733278,\n",
       " 0.3159811453509778,\n",
       " 0.3156940170675244,\n",
       " 0.31541039465202225,\n",
       " 0.31513021899070065,\n",
       " 0.3148534322883914,\n",
       " 0.31457997803129123,\n",
       " 0.31430980095100186,\n",
       " 0.3140428469897974,\n",
       " 0.3137790632670705,\n",
       " 0.31351839804690845,\n",
       " 0.3132608007067562,\n",
       " 0.31300622170712206,\n",
       " 0.31275461256228654,\n",
       " 0.3125059258119741,\n",
       " 0.3122601149939516,\n",
       " 0.31201713461751635,\n",
       " 0.3117769401378416,\n",
       " 0.31153948793114516,\n",
       " 0.3113047352706518,\n",
       " 0.3110726403033168,\n",
       " 0.3108431620272861,\n",
       " 0.31061626027006156,\n",
       " 0.3103918956673488,\n",
       " 0.31017002964256,\n",
       " 0.3099506243869493,\n",
       " 0.3097336428403569,\n",
       " 0.30951904867254065,\n",
       " 0.30930680626507306,\n",
       " 0.3090968806937844,\n",
       " 0.3088892377117324,\n",
       " 0.30868384373267915,\n",
       " 0.3084806658150583,\n",
       " 0.30827967164641507,\n",
       " 0.30808082952830246,\n",
       " 0.3078841083616182,\n",
       " 0.307689477632367,\n",
       " 0.30749690739783364,\n",
       " 0.3073063682731534,\n",
       " 0.3071178314182652,\n",
       " 0.3069312685252363,\n",
       " 0.30674665180594407,\n",
       " 0.3065639539801048,\n",
       " 0.30638314826363666,\n",
       " 0.3062042083573471,\n",
       " 0.30602710843593245,\n",
       " 0.305851823137281,\n",
       " 0.30567832755206914,\n",
       " 0.30550659721364026,\n",
       " 0.3053366080881595,\n",
       " 0.30516833656503267,\n",
       " 0.30500175944758345,\n",
       " 0.30483685394397914,\n",
       " 0.3046735976583979,\n",
       " 0.3045119685824296,\n",
       " 0.30435194508670255,\n",
       " 0.30419350591273076,\n",
       " 0.30403663016497195,\n",
       " 0.3038812973030937,\n",
       " 0.3037274871344377,\n",
       " 0.30357517980667814,\n",
       " 0.3034243558006682,\n",
       " 0.3032749959234682,\n",
       " 0.30312708130155064,\n",
       " 0.30298059337417665,\n",
       " 0.3028355138869382,\n",
       " 0.3026918248854625,\n",
       " 0.30254950870927233,\n",
       " 0.3024085479857985,\n",
       " 0.3022689256245403,\n",
       " 0.302130624811368,\n",
       " 0.3019936290029666,\n",
       " 0.3018579219214125,\n",
       " 0.3017234875488825,\n",
       " 0.3015903101224908,\n",
       " 0.30145837412924825,\n",
       " 0.30132766430114394,\n",
       " 0.3011981656103422,\n",
       " 0.30106986326449414,\n",
       " 0.30094274270215976,\n",
       " 0.30081678958833663,\n",
       " 0.30069198981009465,\n",
       " 0.30056832947231055,\n",
       " 0.3004457948935031,\n",
       " 0.3003243726017624,\n",
       " 0.3002040493307752,\n",
       " 0.3000848120159384,\n",
       " 0.29996664779056353,\n",
       " 0.29984954398216596,\n",
       " 0.29973348810883815,\n",
       " 0.299618467875705,\n",
       " 0.2995044711714588,\n",
       " 0.2993914860649699,\n",
       " 0.2992795008019754,\n",
       " 0.2991685038018388,\n",
       " 0.2990584836543822,\n",
       " 0.2989494291167881,\n",
       " 0.29884132911056877,\n",
       " 0.2987341727186017,\n",
       " 0.2986279491822293,\n",
       " 0.2985226478984216,\n",
       " 0.29841825841699954,\n",
       " 0.2983147704379188,\n",
       " 0.29821217380861054,\n",
       " 0.2981104585213793,\n",
       " 0.2980096147108562,\n",
       " 0.2979096326515059,\n",
       " 0.2978105027551857,\n",
       " 0.2977122155687561,\n",
       " 0.2976147617717413,\n",
       " 0.2975181321740382,\n",
       " 0.2974223177136732,\n",
       " 0.29732730945460484,\n",
       " 0.29723309858457214,\n",
       " 0.297139676412987,\n",
       " 0.29704703436886926,\n",
       " 0.296955163998824,\n",
       " 0.29686405696506035,\n",
       " 0.2967737050434498,\n",
       " 0.2966841001216233,\n",
       " 0.2965952341971072,\n",
       " 0.2965070993754966,\n",
       " 0.2964196878686638,\n",
       " 0.2963329919930043,\n",
       " 0.29624700416771543,\n",
       " 0.2961617169131105,\n",
       " 0.2960771228489652,\n",
       " 0.29599321469289663,\n",
       " 0.295909985258774,\n",
       " 0.2958274274551601,\n",
       " 0.2957455342837831,\n",
       " 0.2956642988380376,\n",
       " 0.29558371430151503,\n",
       " 0.2955037739465616,\n",
       " 0.2954244711328641,\n",
       " 0.29534579930606286,\n",
       " 0.29526775199639105,\n",
       " 0.29519032281733865,\n",
       " 0.2951135054643434,\n",
       " 0.29503729371350507,\n",
       " 0.2949616814203239,\n",
       " 0.29488666251846346,\n",
       " 0.29481223101853543,\n",
       " 0.29473838100690786,\n",
       " 0.2946651066445346,\n",
       " 0.2945924021658074,\n",
       " 0.29452026187742764,\n",
       " 0.2944486801573005,\n",
       " 0.2943776514534475,\n",
       " 0.2943071702829403,\n",
       " 0.29423723123085305,\n",
       " 0.29416782894923377,\n",
       " 0.29409895815609466,\n",
       " 0.2940306136344199,\n",
       " 0.29396279023119215,\n",
       " 0.29389548285643535,\n",
       " 0.29382868648227567,\n",
       " 0.2937623961420179,\n",
       " 0.2936966069292397,\n",
       " 0.29363131399690046,\n",
       " 0.29356651255646615,\n",
       " 0.2935021978770507,\n",
       " 0.2934383652845703,\n",
       " 0.2933750101609144,\n",
       " 0.29331212794313,\n",
       " 0.29324971412262046,\n",
       " 0.293187764244358,\n",
       " 0.2931262739061098,\n",
       " 0.29306523875767776,\n",
       " 0.29300465450015045,\n",
       " 0.29294451688516887,\n",
       " 0.2928848217142039,\n",
       " 0.29282556483784616,\n",
       " 0.2927667421551086,\n",
       " 0.2927083496127396,\n",
       " 0.292650383204549,\n",
       " 0.29259283897074423,\n",
       " 0.29253571299727854,\n",
       " 0.2924790014152093,\n",
       " 0.29242270040006785,\n",
       " 0.2923668061712386,\n",
       " 0.2923113149913499,\n",
       " 0.29225622316567357,\n",
       " 0.29220152704153535,\n",
       " 0.2921472230077343,\n",
       " 0.29209330749397233,\n",
       " 0.2920397769702923,\n",
       " 0.2919866279465263,\n",
       " 0.29193385697175156,\n",
       " 0.2918814606337562,\n",
       " 0.29182943555851387,\n",
       " 0.29177777840966523,\n",
       " 0.2917264858880094,\n",
       " 0.2916755547310032,\n",
       " 0.2916249817122675,\n",
       " 0.2915747636411025,\n",
       " 0.29152489736201037,\n",
       " 0.2914753797542252,\n",
       " 0.2914262077312505,\n",
       " 0.2913773782404041,\n",
       " 0.2913288882623706,\n",
       " 0.2912807348107596,\n",
       " 0.29123291493167236,\n",
       " 0.29118542570327405,\n",
       " 0.2911382642353736,\n",
       " 0.2910914276690087,\n",
       " 0.2910449131760394,\n",
       " 0.29099871795874543,\n",
       " 0.2909528392494318,\n",
       " 0.29090727431003943,\n",
       " 0.29086202043176174,\n",
       " 0.29081707493466774,\n",
       " 0.2907724351673302,\n",
       " 0.29072809850645964,\n",
       " 0.2906840623565444,\n",
       " 0.2906403241494952,\n",
       " 0.2905968813442961,\n",
       " 0.2905537314266595,\n",
       " 0.2905108719086882,\n",
       " 0.2904683003285399,\n",
       " 0.29042601425009945,\n",
       " 0.2903840112626541,\n",
       " 0.2903422889805741,\n",
       " 0.29030084504299863,\n",
       " 0.29025967711352546,\n",
       " 0.29021878287990566,\n",
       " 0.2901781600537431,\n",
       " 0.29013780637019776,\n",
       " 0.2900977195876937,\n",
       " 0.29005789748763133,\n",
       " 0.2900183378741037,\n",
       " 0.28997903857361734,\n",
       " 0.28993999743481674,\n",
       " 0.28990121232821264,\n",
       " 0.28986268114591496,\n",
       " 0.2898244018013689,\n",
       " 0.28978637222909553,\n",
       " 0.28974859038443523,\n",
       " 0.28971105424329524,\n",
       " 0.2896737618019015,\n",
       " 0.28963671107655253,\n",
       " 0.2895999001033783,\n",
       " 0.2895633269381016,\n",
       " 0.289526989655803,\n",
       " 0.2894908863506894,\n",
       " 0.2894550151358656,\n",
       " 0.28941937414310903,\n",
       " 0.2893839615226477,\n",
       " 0.2893487754429417,\n",
       " 0.2893138140904668,\n",
       " 0.28927907566950195,\n",
       " 0.28924455840191937,\n",
       " 0.2892102605269774,\n",
       " 0.2891761803011168,\n",
       " 0.2891423159977592,\n",
       " 0.28910866590710854,\n",
       " 0.2890752283359554,\n",
       " 0.2890420016074841,\n",
       " 0.289008984061082,\n",
       " 0.2889761740521517,\n",
       " 0.28894356995192605,\n",
       " 0.28891117014728507,\n",
       " 0.2888789730405758,\n",
       " 0.28884697704943446,\n",
       " 0.2888151806066113,\n",
       " 0.288783582159797,\n",
       " 0.2887521801714526,\n",
       " 0.2887209731186404,\n",
       " 0.2886899594928585,\n",
       " 0.28865913779987606,\n",
       " 0.2886285065595724,\n",
       " 0.2885980643057767,\n",
       " 0.28856780958611106,\n",
       " 0.28853774096183443,\n",
       " 0.28850785700769005,\n",
       " 0.2884781563117537,\n",
       " 0.28844863747528443,\n",
       " 0.2884192991125773,\n",
       " 0.28839013985081785,\n",
       " 0.2883611583299387,\n",
       " 0.28833235320247796,\n",
       " 0.2883037231334393,\n",
       " 0.2882752668001541,\n",
       " 0.28824698289214523,\n",
       " 0.2882188701109927,\n",
       " 0.2881909271702008,\n",
       " 0.28816315279506755,\n",
       " 0.28813554572255506,\n",
       " 0.28810810470116205,\n",
       " 0.2880808284907979,\n",
       " 0.28805371586265843,\n",
       " 0.28802676559910295,\n",
       " 0.2879999764935331,\n",
       " 0.28797334735027325,\n",
       " 0.2879468769844524,\n",
       " 0.2879205642218876,\n",
       " 0.28789440789896864,\n",
       " 0.2878684068625445,\n",
       " 0.28784255996981095,\n",
       " 0.28781686608819973,\n",
       " 0.28779132409526936,\n",
       " 0.28776593287859675,\n",
       " 0.28774069133567054,\n",
       " 0.28771559837378563,\n",
       " 0.28769065290993934,\n",
       " 0.28766585387072846,\n",
       " 0.2876412001922477,\n",
       " 0.28761669081998964,\n",
       " 0.2875923247087453,\n",
       " 0.2875681008225067,\n",
       " 0.28754401813437036,\n",
       " 0.28752007562644155,\n",
       " 0.28749627228974045,\n",
       " 0.2874726071241084,\n",
       " 0.287449079138117,\n",
       " 0.28742568734897644,\n",
       " 0.28740243078244604,\n",
       " 0.28737930847274584,\n",
       " 0.28735631946246887,\n",
       " 0.28733346280249455,\n",
       " 0.2873107375519036,\n",
       " 0.2872881427778933,\n",
       " 0.2872656775556942,\n",
       " 0.2872433409684882,\n",
       " 0.28722113210732614,\n",
       " 0.2871990500710484,\n",
       " 0.28717709396620483,\n",
       " 0.2871552629069767,\n",
       " 0.28713355601509843,\n",
       " 0.2871119724197818,\n",
       " 0.28709051125763957,\n",
       " 0.2870691716726108,\n",
       " 0.28704795281588724,\n",
       " 0.2870268538458395,\n",
       " 0.2870058739279457,\n",
       " 0.2869850122347194,\n",
       " 0.28696426794563945,\n",
       " 0.2869436402470802,\n",
       " 0.2869231283322425,\n",
       " 0.28690273140108574,\n",
       " 0.2868824486602604,\n",
       " 0.2868622793230418,\n",
       " 0.2868422226092641,\n",
       " 0.28682227774525554,\n",
       " 0.28680244396377397,\n",
       " 0.2867827205039436,\n",
       " 0.28676310661119203,\n",
       " 0.28674360153718853,\n",
       " 0.2867242045397825,\n",
       " 0.28670491488294275,\n",
       " 0.286685731836698,\n",
       " 0.28666665467707725,\n",
       " 0.2866476826860512,\n",
       " 0.2866288151514749,\n",
       " 0.2866100513670299,\n",
       " 0.286591390632168,\n",
       " 0.2865728322520552,\n",
       " 0.2865543755375166,\n",
       " 0.2865360198049812,\n",
       " 0.28651776437642873,\n",
       " 0.28649960857933504,\n",
       " 0.28648155174662043,\n",
       " 0.28646359321659653,\n",
       " 0.2864457323329149,\n",
       " 0.28642796844451623,\n",
       " 0.2864103009055793,\n",
       " 0.28639272907547125,\n",
       " 0.2863752523186984,\n",
       " 0.286357870004857,\n",
       " 0.2863405815085853,\n",
       " 0.28632338620951525,\n",
       " 0.28630628349222587,\n",
       " 0.286289272746196,\n",
       " 0.2862723533657584,\n",
       " 0.28625552475005395,\n",
       " 0.2862387863029864,\n",
       " 0.2862221374331777,\n",
       " 0.28620557755392373,\n",
       " 0.2861891060831507,\n",
       " 0.2861727224433717,\n",
       " 0.28615642606164404,\n",
       " 0.2861402163695269,\n",
       " 0.2861240928030395,\n",
       " 0.2861080548026194,\n",
       " 0.2860921018130821,\n",
       " 0.28607623328358,\n",
       " 0.2860604486675625,\n",
       " 0.2860447474227365,\n",
       " 0.28602912901102695,\n",
       " 0.2860135928985381,\n",
       " 0.28599813855551515,\n",
       " 0.2859827654563063,\n",
       " 0.2859674730793251,\n",
       " 0.2859522609070133,\n",
       " 0.28593712842580415,\n",
       " 0.28592207512608586,\n",
       " 0.28590710050216583,\n",
       " 0.2858922040522347,\n",
       " 0.28587738527833134,\n",
       " 0.28586264368630815,\n",
       " 0.28584797878579604,\n",
       " 0.28583339009017095,\n",
       " 0.2858188771165193,\n",
       " 0.2858044393856054,\n",
       " 0.2857900764218376,\n",
       " 0.28577578775323625,\n",
       " 0.2857615729114007,\n",
       " 0.2857474314314778,\n",
       " 0.28573336285212964,\n",
       " 0.28571936671550263,\n",
       " 0.28570544256719627,\n",
       " 0.2856915899562323,\n",
       " 0.28567780843502444,\n",
       " 0.2856640975593482,\n",
       " 0.28565045688831114,\n",
       " 0.2856368859843233,\n",
       " 0.2856233844130681,\n",
       " 0.2856099517434734,\n",
       " 0.2855965875476831,\n",
       " 0.2855832914010282,\n",
       " 0.28557006288200004,\n",
       " 0.2855569015722211,\n",
       " 0.2855438070564187,\n",
       " 0.28553077892239714,\n",
       " 0.2855178167610113,\n",
       " 0.2855049201661399,\n",
       " 0.2854920887346587,\n",
       " 0.28547932206641524,\n",
       " 0.28546661976420235,\n",
       " 0.2854539814337331,\n",
       " 0.28544140668361484,\n",
       " 0.28542889512532493,\n",
       " 0.2854164463731856,\n",
       " 0.2854040600443392,\n",
       " 0.28539173575872434,\n",
       " 0.2853794731390515,\n",
       " 0.2853672718107794,\n",
       " 0.28535513140209146,\n",
       " 0.2853430515438722,\n",
       " 0.2853310318696844,\n",
       " 0.2853190720157462,\n",
       " 0.2853071716209083,\n",
       " 0.2852953303266316,\n",
       " 0.2852835477769652,\n",
       " 0.285271823618524,\n",
       " 0.2852601575004673,\n",
       " 0.2852485490744771,\n",
       " 0.2852369979947368,\n",
       " 0.28522550391791,\n",
       " 0.28521406650311965,\n",
       " 0.2852026854119271,\n",
       " 0.2851913603083121,\n",
       " 0.2851800908586521,\n",
       " 0.28516887673170177,\n",
       " 0.2851577175985739,\n",
       " 0.2851466131327194,\n",
       " 0.2851355630099071,\n",
       " 0.2851245669082054,\n",
       " 0.28511362450796235,\n",
       " 0.285102735491787,\n",
       " 0.28509189954453074,\n",
       " 0.2850811163532682,\n",
       " 0.28507038560727926,\n",
       " 0.2850597069980308,\n",
       " 0.2850490802191581,\n",
       " 0.2850385049664477,\n",
       " 0.28502798093781895,\n",
       " 0.28501750783330704,\n",
       " 0.2850070853550448,\n",
       " 0.2849967132072467,\n",
       " 0.28498639109619023,\n",
       " 0.28497611873020057,\n",
       " 0.28496589581963255,\n",
       " 0.2849557220768547,\n",
       " 0.284945597216233,\n",
       " 0.2849355209541139,\n",
       " 0.28492549300880887,\n",
       " 0.28491551310057794,\n",
       " 0.2849055809516143,\n",
       " 0.28489569628602834,\n",
       " 0.28488585882983225,\n",
       " 0.2848760683109244,\n",
       " 0.2848663244590746,\n",
       " 0.28485662700590847,\n",
       " 0.28484697568489276,\n",
       " 0.2848373702313205,\n",
       " 0.2848278103822964,\n",
       " 0.2848182958767221,\n",
       " 0.2848088264552818,\n",
       " 0.28479940186042835,\n",
       " 0.2847900218363687,\n",
       " 0.2847806861290499,\n",
       " 0.28477139448614547,\n",
       " 0.2847621466570415,\n",
       " 0.2847529423928231,\n",
       " 0.2847437814462607,\n",
       " 0.2847346635717969,\n",
       " 0.28472558852553326,\n",
       " 0.2847165560652167,\n",
       " 0.2847075659502273,\n",
       " 0.28469861794156437,\n",
       " 0.28468971180183483,\n",
       " 0.2846808472952395,\n",
       " 0.2846720241875613,\n",
       " 0.28466324224615247,\n",
       " 0.28465450123992214,\n",
       " 0.28464580093932457,\n",
       " 0.2846371411163465,\n",
       " 0.28462852154449586,\n",
       " 0.28461994199878904,\n",
       " 0.2846114022557399,\n",
       " 0.28460290209334754,\n",
       " 0.284594441291085,\n",
       " 0.284586019629888,\n",
       " 0.28457763689214305,\n",
       " 0.28456929286167676,\n",
       " 0.28456098732374435,\n",
       " 0.2845527200650185,\n",
       " 0.2845444908735789,\n",
       " 0.28453629953890097,\n",
       " 0.28452814585184505,\n",
       " 0.2845200296046462,\n",
       " 0.2845119505909029,\n",
       " 0.28450390860556746,\n",
       " 0.28449590344493497,\n",
       " 0.28448793490663327,\n",
       " 0.2844800027896126,\n",
       " 0.2844721068941357,\n",
       " 0.28446424702176776,\n",
       " 0.2844564229753663,\n",
       " 0.2844486345590714,\n",
       " 0.2844408815782961,\n",
       " 0.28443316383971673,\n",
       " 0.28442548115126287,\n",
       " 0.28441783332210846,\n",
       " 0.28441022016266193,\n",
       " 0.28440264148455724,\n",
       " 0.2843950971006443,\n",
       " 0.2843875868249799,\n",
       " 0.2843801104728185,\n",
       " 0.28437266786060367,\n",
       " 0.2843652588059586,\n",
       " 0.28435788312767735,\n",
       " 0.2843505406457164,\n",
       " 0.28434323118118565,\n",
       " 0.28433595455633986,\n",
       " 0.28432871059457,\n",
       " 0.2843214991203948,\n",
       " 0.2843143199594526,\n",
       " 0.2843071729384926,\n",
       " 0.2843000578853668,\n",
       " 0.2842929746290219,\n",
       " 0.28428592299949096,\n",
       " 0.28427890282788537,\n",
       " 0.2842719139463872,\n",
       " 0.2842649561882406,\n",
       " 0.2842580293877447,\n",
       " 0.2842511333802453,\n",
       " 0.2842442680021273,\n",
       " 0.2842374330908073,\n",
       " 0.2842306284847254,\n",
       " 0.28422385402333833,\n",
       " 0.2842171095471117,\n",
       " 0.2842103948975124,\n",
       " 0.2842037099170017,\n",
       " 0.28419705444902754,\n",
       " 0.2841904283380177,\n",
       " 0.2841838314293725,\n",
       " 0.28417726356945733,\n",
       " 0.28417072460559634,\n",
       " 0.28416421438606476,\n",
       " 0.28415773276008266,\n",
       " 0.28415127957780756,\n",
       " 0.2841448546903276,\n",
       " 0.28413845794965537,\n",
       " 0.2841320892087206,\n",
       " 0.28412574832136384,\n",
       " 0.28411943514232985,\n",
       " 0.2841131495272611,\n",
       " 0.28410689133269096,\n",
       " 0.28410066041603804,\n",
       " 0.28409445663559896,\n",
       " 0.2840882798505426,\n",
       " 0.2840821299209037,\n",
       " 0.2840760067075766,\n",
       " 0.284069910072309,\n",
       " 0.2840638398776963,\n",
       " 0.28405779598717473,\n",
       " 0.2840517782650164,\n",
       " 0.2840457865763223,\n",
       " 0.2840398207870171,\n",
       " 0.2840338807638431,\n",
       " 0.28402796637435423,\n",
       " 0.28402207748691055,\n",
       " 0.28401621397067245,\n",
       " 0.28401037569559473,\n",
       " 0.28400456253242157,\n",
       " 0.2839987743526802,\n",
       " 0.283993011028676,\n",
       " 0.28398727243348676,\n",
       " 0.2839815584409572,\n",
       " 0.2839758689256933,\n",
       " 0.2839702037630578,\n",
       " 0.28396456282916405,\n",
       " 0.2839589460008709,\n",
       " 0.2839533531557779,\n",
       " 0.2839477841722196,\n",
       " 0.28394223892926074,\n",
       " 0.28393671730669096,\n",
       " 0.28393121918501985,\n",
       " 0.2839257444454719,\n",
       " 0.2839202929699816,\n",
       " 0.28391486464118815,\n",
       " 0.283909459342431,\n",
       " 0.2839040769577448,\n",
       " 0.2838987173718546,\n",
       " 0.28389338047017104,\n",
       " 0.2838880661387855,\n",
       " 0.2838827742644656,\n",
       " 0.28387750473465045,\n",
       " 0.283872257437446,\n",
       " 0.28386703226162024,\n",
       " 0.2838618290965991,\n",
       " 0.28385664783246156,\n",
       " 0.2838514883599352,\n",
       " 0.28384635057039176,\n",
       " 0.28384123435584296,\n",
       " 0.28383613960893583,\n",
       " 0.2838310662229483,\n",
       " 0.2838260140917852,\n",
       " 0.2838209831099738,\n",
       " 0.2838159731726594,\n",
       " 0.2838109841756015,\n",
       " 0.28380601601516936,\n",
       " 0.2838010685883378,\n",
       " 0.28379614179268314,\n",
       " 0.28379123552637925,\n",
       " 0.28378634968819344,\n",
       " 0.2837814841774821,\n",
       " 0.28377663889418747,\n",
       " 0.28377181373883276,\n",
       " 0.2837670086125189,\n",
       " 0.28376222341692037,\n",
       " 0.2837574580542813,\n",
       " 0.2837527124274118,\n",
       " 0.283747986439684,\n",
       " 0.28374327999502846,\n",
       " 0.2837385929979298,\n",
       " 0.2837339253534242,\n",
       " 0.2837292769670944,\n",
       " 0.28372464774506667,\n",
       " 0.2837200375940073,\n",
       " 0.2837154464211188,\n",
       " 0.28371087413413604,\n",
       " 0.2837063206413232,\n",
       " 0.2837017858514699,\n",
       " 0.28369726967388786,\n",
       " 0.2836927720184074,\n",
       " 0.2836882927953739,\n",
       " 0.28368383191564445,\n",
       " 0.28367938929058456,\n",
       " 0.28367496483206456,\n",
       " 0.2836705584524564,\n",
       " 0.2836661700646304,\n",
       " 0.2836617995819518,\n",
       " 0.2836574469182775,\n",
       " 0.283653111987953,\n",
       " 0.2836487947058089,\n",
       " 0.28364449498715816,\n",
       " 0.2836402127477923,\n",
       " 0.2836359479039788,\n",
       " 0.2836317003724576,\n",
       " 0.28362747007043826,\n",
       " 0.28362325691559687,\n",
       " 0.2836190608260728,\n",
       " 0.28361488172046573,\n",
       " 0.28361071951783284,\n",
       " 0.28360657413768553,\n",
       " 0.2836024454999868,\n",
       " 0.28359833352514796,\n",
       " 0.28359423813402584,\n",
       " 0.28359015924792,\n",
       " 0.2835860967885699,\n",
       " 0.2835820506781516,\n",
       " 0.28357802083927547,\n",
       " 0.2835740071949832,\n",
       " 0.2835700096687448,\n",
       " 0.283566028184456,\n",
       " 0.2835620626664356,\n",
       " 0.2835581130394226,\n",
       " 0.28355417922857346,\n",
       " 0.2835502611594596,\n",
       " 0.2835463587580644,\n",
       " 0.28354247195078097,\n",
       " 0.28353860066440906,\n",
       " 0.2835347448261529,\n",
       " 0.28353090436361833,\n",
       " 0.28352707920481024,\n",
       " 0.2835232692781301,\n",
       " 0.28351947451237336,\n",
       " 0.2835156948367271,\n",
       " 0.2835119301807672,\n",
       " 0.2835081804744562,\n",
       " 0.2835044456481406,\n",
       " 0.2835007256325485,\n",
       " 0.2834970203587873,\n",
       " 0.283493329758341,\n",
       " 0.28348965376306823,\n",
       " 0.28348599230519933,\n",
       " 0.28348234531733446,\n",
       " 0.283478712732441,\n",
       " 0.2834750944838514,\n",
       " 0.28347149050526066,\n",
       " 0.28346790073072436,\n",
       " 0.283464325094656,\n",
       " 0.28346076353182503,\n",
       " 0.28345721597735457,\n",
       " 0.2834536823667189,\n",
       " 0.2834501626357418,\n",
       " 0.28344665672059377,\n",
       " 0.28344316455779034,\n",
       " 0.28343968608418935,\n",
       " 0.28343622123698964,\n",
       " 0.28343276995372796,\n",
       " 0.28342933217227756,\n",
       " 0.2834259078308459,\n",
       " 0.2834224968679722,\n",
       " 0.28341909922252617,\n",
       " 0.28341571483370487,\n",
       " 0.2834123436410318,\n",
       " 0.283408985584354,\n",
       " 0.28340564060384055,\n",
       " 0.2834023086399803,\n",
       " 0.2833989896335801,\n",
       " 0.2833956835257625,\n",
       " 0.28339239025796426,\n",
       " 0.283389109771934,\n",
       " 0.28338584200973044,\n",
       " 0.28338258691372037,\n",
       " 0.28337934442657725,\n",
       " 0.28337611449127853,\n",
       " 0.28337289705110413,\n",
       " 0.28336969204963497,\n",
       " 0.28336649943075054,\n",
       " 0.28336331913862717,\n",
       " 0.2833601511177367,\n",
       " 0.28335699531284403,\n",
       " 0.28335385166900556,\n",
       " 0.28335072013156765,\n",
       " 0.2833476006461644,\n",
       " 0.2833444931587163,\n",
       " 0.28334139761542826,\n",
       " 0.28333831396278797,\n",
       " 0.283335242147564,\n",
       " 0.2833321821168044,\n",
       " 0.28332913381783453,\n",
       " 0.28332609719825613,\n",
       " 0.28332307220594477,\n",
       " 0.28332005878904876,\n",
       " 0.28331705689598724,\n",
       " 0.28331406647544877,\n",
       " 0.28331108747638944,\n",
       " 0.2833081198480315,\n",
       " 0.28330516353986146,\n",
       " 0.2833022185016287,\n",
       " 0.283299284683344,\n",
       " 0.2832963620352776,\n",
       " 0.2832934505079579,\n",
       " 0.28329055005217,\n",
       " 0.2832876606189539,\n",
       " 0.28328478215960295,\n",
       " 0.2832819146256626,\n",
       " 0.2832790579689289,\n",
       " 0.2832762121414464,\n",
       " 0.2832733770955076,\n",
       " 0.2832705527836506,\n",
       " 0.2832677391586582,\n",
       " 0.2832649361735562,\n",
       " 0.283262143781612,\n",
       " 0.28325936193633316,\n",
       " 0.2832565905914659,\n",
       " 0.2832538297009939,\n",
       " 0.28325107921913656,\n",
       " 0.2832483391003478,\n",
       " 0.28324560929931475,\n",
       " 0.28324288977095613,\n",
       " 0.28324018047042115,\n",
       " 0.2832374813530878,\n",
       " 0.2832347923745619,\n",
       " 0.28323211349067545,\n",
       " 0.2832294446574853,\n",
       " 0.2832267858312721,\n",
       " 0.28322413696853876,\n",
       " 0.28322149802600904,\n",
       " 0.28321886896062654,\n",
       " 0.2832162497295532,\n",
       " 0.28321364029016804,\n",
       " 0.28321104060006597,\n",
       " 0.2832084506170564,\n",
       " 0.2832058702991621,\n",
       " 0.283203299604618,\n",
       " 0.28320073849186966,\n",
       " 0.2831981869195723,\n",
       " 0.28319564484658966,\n",
       " 0.2831931122319923,\n",
       " 0.2831905890350571,\n",
       " 0.28318807521526546,\n",
       " 0.2831855707323022,\n",
       " 0.28318307554605493,\n",
       " 0.283180589616612,\n",
       " 0.28317811290426204,\n",
       " 0.28317564536949236,\n",
       " 0.2831731869729882,\n",
       " 0.28317073767563117,\n",
       " 0.2831682974384983,\n",
       " 0.28316586622286105,\n",
       " 0.28316344399018395,\n",
       " 0.28316103070212356,\n",
       " 0.28315862632052746,\n",
       " 0.28315623080743296,\n",
       " 0.2831538441250662,\n",
       " 0.283151466235841,\n",
       " 0.28314909710235775,\n",
       " 0.2831467366874022,\n",
       " 0.2831443849539447,\n",
       " 0.283142041865139,\n",
       " 0.28313970738432115,\n",
       " 0.2831373814750084,\n",
       " 0.28313506410089834,\n",
       " 0.28313275522586767,\n",
       " 0.2831304548139715,\n",
       " 0.2831281628294418,\n",
       " 0.2831258792366871,\n",
       " 0.28312360400029074,\n",
       " 0.2831213370850104,\n",
       " 0.28311907845577694,\n",
       " 0.2831168280776933,\n",
       " 0.28311458591603367,\n",
       " 0.28311235193624257,\n",
       " 0.2831101261039336,\n",
       " 0.28310790838488886,\n",
       " 0.28310569874505775,\n",
       " 0.283103497150556,\n",
       " 0.28310130356766483,\n",
       " 0.2830991179628301,\n",
       " 0.2830969403026612,\n",
       " 0.28309477055393006,\n",
       " 0.28309260868357067,\n",
       " 0.28309045465867766]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Try different learning rates and compare the results. How does the learning rate influence the convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Try different regularization parameter values and compare the model quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Compare zero initialization and random initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you need to implement weighted K-Neighbors Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that training a KNN classifier is simply memorizing a training sample. \n",
    "\n",
    "The process of applying a classifier for one object is to find the distances from it to all objects in the training data, then select the k nearest objects (neighbors) and return the most common class among these objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also give the nearest neighbors weights in accordance with the distance of the object to them. In the simplest case (as in your assignment), you can set the weights inversely proportional to that distance. \n",
    "\n",
    "$$w_{i} = \\frac{1}{d_{i} + eps},$$\n",
    "\n",
    "where $d_{i}$ is the distance between object and i-th nearest neighbor and $eps$ is the small value to prevent division by zero.\n",
    "\n",
    "In case of 'uniform' weights, all k nearest neighbors are equivalent (have equal weight, for example $w_{i} = 1, \\forall i \\in(1,k)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the probability of classes, it is necessary to normalize the weights of each class, dividing them by the sum:\n",
    "\n",
    "$$p_{i} = \\frac{w_{i}}{\\sum_{j=1}^{c}w_{j}},$$\n",
    "\n",
    "where $p_i$ is probability of i-th class and $c$ is the number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2 points)** Implement the algorithm and use it to classify the digits. By implementing this algorithm, you will be able to classify numbers not only into \"even\" or \"odd\", but into their real representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomKNeighborsClassifier:\n",
    "    _estimator_type = \"classifier\"\n",
    "    \n",
    "    def __init__(self, n_neighbors=5, weights='uniform', eps=1e-9):\n",
    "        \"\"\"K-Nearest Neighbors classifier.\n",
    "        \n",
    "        Args:\n",
    "            n_neighbors: int, default=5\n",
    "                Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
    "            weights : {'uniform', 'distance'} or callable, default='uniform'\n",
    "                Weight function used in prediction.  Possible values:\n",
    "                - 'uniform' : uniform weights.  All points in each neighborhood\n",
    "                  are weighted equally.\n",
    "                - 'distance' : weight points by the inverse of their distance.\n",
    "                  in this case, closer neighbors of a query point will have a\n",
    "                  greater influence than neighbors which are further away.\n",
    "            eps : float, default=1e-5\n",
    "                Epsilon to prevent division by 0 \n",
    "        \"\"\"\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.weights = weights\n",
    "        self.eps = eps\n",
    "        \n",
    "    \n",
    "    def get_pairwise_distances(self, X, Y):\n",
    "        \"\"\"\n",
    "        Returnes matrix of the pairwise distances between the rows from both X and Y.\n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "            Y: numpy array of shape (k_samples, n_features)\n",
    "        Returns:\n",
    "            P: numpy array of shape (n_samples, k_samples)\n",
    "                Matrix in which (i, j) value is the distance \n",
    "                between i'th row from the X and j'th row from the Y.\n",
    "        \"\"\"\n",
    "        # <your code>\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def get_class_weights(self, y, weights):\n",
    "        \"\"\"\n",
    "        Returns a vector with sum of weights for each class \n",
    "        Args:\n",
    "            y: numpy array of shape (n_samles,)\n",
    "            weights: numpy array of shape (n_samples,)\n",
    "                The weights of the corresponding points of y.\n",
    "        Returns:\n",
    "            p: numpy array of shape (n_classes)\n",
    "                Array where the value at the i-th position \n",
    "                corresponds to the weight of the i-th class.\n",
    "        \"\"\"\n",
    "        # <your code>\n",
    "        pass\n",
    "            \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Target vector.        \n",
    "        \"\"\"\n",
    "        self.points = X\n",
    "        self.y = y\n",
    "        self.classes_ = np.unique(y)\n",
    "        \n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict positive class probabilities.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            y: numpy array of shape (n_samples, n_classes)\n",
    "                Vector containing positive class probabilities.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'points'):\n",
    "            P = self.get_pairwise_distances(X, self.points)\n",
    "            \n",
    "            weights_of_points = np.ones(P.shape)\n",
    "            if self.weights == 'distance':\n",
    "                weights_of_points = 'your code'\n",
    "                \n",
    "            # <your code>\n",
    "            pass\n",
    "        \n",
    "        else: \n",
    "            raise NotFittedError(\"CustomKNeighborsClassifier instance is not fitted yet\")\n",
    "            \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Vector containing predicted class labels.\n",
    "        \"\"\"\n",
    "        # <your code>\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomKNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13468/2313085331.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m assert np.allclose(model.get_pairwise_distances(np.array([[0  , 1]  , [1, 1]]), \n\u001b[0m\u001b[0;32m      2\u001b[0m                                                 np.array([[0.5, 0.5], [1, 0]])),\n\u001b[0;32m      3\u001b[0m                    np.array([[0.70710678, 1.41421356],\n\u001b[0;32m      4\u001b[0m                              [0.70710678, 1.        ]]))\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mallclose\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mD:\\MiniConda\\envs\\rsschool-machine-learning-course\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mallclose\u001b[1;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[0;32m   2247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2248\u001b[0m     \"\"\"\n\u001b[1;32m-> 2249\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0matol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mequal_nan\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mequal_nan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2250\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36misclose\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mD:\\MiniConda\\envs\\rsschool-machine-learning-course\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36misclose\u001b[1;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[0;32m   2353\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2355\u001b[1;33m     \u001b[0mxfin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2356\u001b[0m     \u001b[0myfin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxfin\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myfin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "assert np.allclose(model.get_pairwise_distances(np.array([[0  , 1]  , [1, 1]]), \n",
    "                                                np.array([[0.5, 0.5], [1, 0]])),\n",
    "                   np.array([[0.70710678, 1.41421356],\n",
    "                             [0.70710678, 1.        ]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classes_ = ['one', 'two', 'three']\n",
    "assert np.allclose(model.get_class_weights(np.array(['one', 'one', 'three', 'two']), np.array([1, 1, 0, 4])), \n",
    "                   np.array([2,4,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_digits(n_class=10, return_X_y=True)\n",
    "\n",
    "_, axes = plt.subplots(nrows=3, ncols=7, figsize=(10, 5))\n",
    "for ax, image, label in zip(axes.flatten(), X, y):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image.reshape((8, 8)), cmap=plt.cm.gray_r if label % 2 else plt.cm.afmhot_r)\n",
    "    ax.set_title(label)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "knn.fit(X_train, list(map(str, y_train)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(model.predict_proba(X_test), knn.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, test_acc = fit_evaluate(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_acc == 1\n",
    "assert test_acc > 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Take a look at the confusion matrix and tell what numbers the model confuses and why this happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Try different n_neighbors parameters and compare the output probabilities of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Compare both 'uniform' and 'distance' weights and share your thoughts in what situations which parameter can be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Suggest another distance measurement function that could improve the quality of the classification for this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Suggest different task and distance function that you think would be suitable for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Synthetic Titanic Survival Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Read the description here: https://www.kaggle.com/c/tabular-playground-series-apr-2021/data. Download the dataset and place it in the *data/titanic/* folder in your working directory.\n",
    "You will use train.csv for model training and validation. The test set is used for model testing: once the model is trained, you can predict whether a passenger survived or not for each passenger in the test set, and submit the predictions: https://www.kaggle.com/c/tabular-playground-series-apr-2021/overview/evaluation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(PATH, 'titanic', 'train.csv')).set_index('PassengerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** How many females and males are there in the dataset? What about the survived passengers? Is there any relationship between the gender and the survival?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Sex.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='Sex', data=data)\n",
    "plt.title('Distribution of Sex between male and female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precisely, {100*data.Sex.value_counts()[0]/len(data)}% were male and {100*data.Sex.value_counts()[1]/len(data)}% were female\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Plot age distribution of the passengers. What is the average and the median age of survived and deceased passengers? Do age distributions differ for survived and deceased passengers? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 point)** Explore \"passenger class\" and \"embarked\" features. What class was \"the safest\"? Is there any relationship between the embarkation port and the survival? Provide the corresponding visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Find the percentage of missing values for each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about the ways to handle these missing values for modelling and write your answer below. Which methods would you suggest? What are their advantages and disadvantages?\n",
    "\n",
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.5 points)** Prepare the features and train two models (KNN and Logistic Regression) to predict the survival. Compare the results. Use accuracy as a metric. Don't forget about cross-validation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 + X points)** Try more feature engineering and hyperparameter tuning to improve the results. You may use either KNN or Logistic Regression (or both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the best model, load the test set and make the predictions. Submit them to kaggle and see the results :)\n",
    "\n",
    "**Note**. X points will depend on your kaggle public leaderboard score.\n",
    "$$ f(score) = 1.0, \\ \\ 0.79 \\leq score < 0.80,$$\n",
    "$$ f(score) = 2.5, \\ \\ 0.80 \\leq score < 0.81,$$ \n",
    "$$ f(score) = 4.0, \\ \\ 0.81 \\leq score $$ \n",
    "Your code should generate the output submitted to kaggle. Fix random seeds to make the results reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
